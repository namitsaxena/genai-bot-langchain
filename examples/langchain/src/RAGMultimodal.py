from vertexai.preview.generative_models import (
    GenerativeModel,
)
from vertexai.language_models import TextEmbeddingModel
from utils.intro_multimodal_rag_utils import get_document_metadata

from utils.intro_multimodal_rag_utils import (
    get_similar_text_from_query,
    print_text_to_text_citation,
    get_similar_image_from_query,
    print_text_to_image_citation,
    get_gemini_response,
    display_images,
)

class MultiModalRAG:
    """
    https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb
    """

    def __init__(self, project_id, generative_model="gemini-pro-vision", text_embedding_model="textembedding-gecko@001"):
        self.project_id = project_id
        self.model = GenerativeModel(generative_model)
        self.text_embedding_model = TextEmbeddingModel.from_pretrained(text_embedding_model)
        self.text_metadata_df = None
        self.image_metadata_df = None


    def process_document_metadata(self, pdf_path="google-10k-sample-14pages.pdf", output_img_dir="../resources/images"):
        # Specify the image description prompt. Change it
        image_description_prompt = """Explain what is going on in the image.
        If it's a table, extract all elements of the table.
        If it's a graph, explain the findings in the graph.
        Do not include any numbers that are not mentioned in the image:"""

        # Extract text and image metadata from the PDF document
        self.text_metadata_df, self.image_metadata_df = get_document_metadata(
            self.project_id,
            self.model,
            pdf_path,
            image_save_dir=output_img_dir,
            image_description_prompt=image_description_prompt,
            embedding_size=1408,
            text_emb_text_limit=1000,  # Set text embedding input text limit to 1000 char
        )
        print("--- Completed processing. ---")
        return self.text_metadata_df, self.image_metadata_df

    def search_text(self, query):
        """
        simple text search using text embeddings
        :param query:
        :return:
        """
        matching_results_text = get_similar_text_from_query(
            self.project_id,
            query,
            self.text_metadata_df,
            column_name="text_embedding_chunk",
            top_n=3,
            embedding_size=1408,
            chunk_text=True,
        )

        # Print the matched text citations
        print_text_to_text_citation(matching_results_text, print_top=True, chunk_text=True)
        return matching_results_text

    def search_image(self, query):
        """
        simple image search using image embeddings
        :param query:
        :return:
        """
        matching_results_image = get_similar_image_from_query(
            self.project_id,
            self.text_metadata_df,
            self.image_metadata_df,
            query=query,
            column_name="text_embedding_from_image_description",  # Use image description text embedding
            image_emb=False,  # Use text embedding instead of image embedding
            top_n=3,
            embedding_size=1408,
        )

        # Markdown(print_text_to_image_citation(matching_results_image, print_top=True))
        print("\n **** Result: ***** \n")

        # Display the top matching image
        # display(matching_results_image[0]["image_object"])
        return matching_results_image

    def perform_rag(self, query):
        print("RAG processing...")
        ############################################
        # get all relevant text chunks and images
        ############################################
        matching_results_chunks_data = self.search_text(query)
        matching_results_image_fromdescription_data = self.search_image(query)

        ############################################
        # create context text and context images
        ############################################
        # combine all the selected relevant text chunks
        context_text = []
        for key, value in matching_results_chunks_data.items():
            context_text.append(value["chunk_text"])
        final_context_text = "\n".join(context_text)
        print(f"Context Text: {context_text}")

        # combine all the relevant images and their description generated by Gemini
        context_images = []
        for key, value in matching_results_image_fromdescription_data.items():
            context_images.extend(
                ["Image: ", value["image_object"], "Caption: ", value["image_description"]]
            )

        ############################################
        # Pass context to Gemini
        ############################################
        instructions = """The context of extraction of detail should be based on the text context given in "text_context" and Image context given in "image_context" along with its Caption: \n
        Base your response on "text_context" and "image_context". Do not use any numbers or percentages that are not present in the "image_context".
        Do not include any cumulative total return in the answer. Context:
        """

        final_prompt = [
            query,
            instructions,
            "text_context:",
            "\n".join(context_text),
            "image_context:",
        ]
        final_prompt.extend(context_images)

        response = get_gemini_response(self.model, model_input=final_prompt, stream=True)
        print(f"Response: {response}")

        # Image citations. You can check how Gemini generated metadata helped in grounding the answer.
        print("Image Citations:-")
        print_text_to_image_citation(
            matching_results_image_fromdescription_data, print_top=False
        )

        # Text citations
        print("Text Citations:-")
        print_text_to_text_citation(
            matching_results_chunks_data,
            print_top=False,
            chunk_text=True,
        )